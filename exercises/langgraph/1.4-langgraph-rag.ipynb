{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9af2cedbcec94d",
   "metadata": {},
   "source": [
    "# LangGraph - RAG Exercise! üöÄ\n",
    "\n",
    "<a target=\"_blank\" href=\"https://githubtocolab.com/IT-HUSET/ai-agenter-2025/blob/main/exercises/langgraph/1.4-langgraph-rag.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>\n",
    "\n",
    "Let's add some RAG to LangGraph!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf534f63f3d850",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f94673c1cabdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T09:57:33.943163Z",
     "start_time": "2025-01-29T09:57:25.460506Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai~=2.0 httpx~=0.28.1 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 docarray~=0.41.0 pypdf~=6.1 --upgrade --quiet\n",
    "%pip install chromadb~=1.1.1 lark~=1.3 --upgrade --quiet\n",
    "%pip install langchain~=0.3 langchain_openai~=0.3 langchain_community~=0.3.31 langchain-chroma~=0.2.6 --upgrade --quiet\n",
    "%pip install langgraph~=0.6 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d510e51c0909d92",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085d7c17c7156a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T09:57:33.953505Z",
     "start_time": "2025-01-29T09:57:33.946343Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IN_COLAB = True\n",
    "    # Get API key from Colab secrets\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ Running in Google Colab - API key loaded from secrets\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    # Load from .env file for local development\n",
    "    try:\n",
    "        from dotenv import load_dotenv, find_dotenv\n",
    "        load_dotenv(find_dotenv())\n",
    "        print(\"‚úÖ Running locally - API key loaded from .env file\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ùå OPENAI_API_KEY not found!\")\n",
    "    if IN_COLAB:\n",
    "        print(\"   ‚Üí Click the key icon (üîë) in the left sidebar\")\n",
    "        print(\"   ‚Üí Add a secret named 'OPENAI_API_KEY'\")\n",
    "        print(\"   ‚Üí Toggle 'Notebook access' to enable it\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Create a .env file with: OPENAI_API_KEY=your-key-here\")\n",
    "else:\n",
    "    print(\"‚úÖ API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5192ae79a33f11",
   "metadata": {},
   "source": [
    "### Setup Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6092d8a5ac3984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T09:57:34.683064Z",
     "start_time": "2025-01-29T09:57:34.026451Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e945bc7bc2583e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "116bb16b204d989b",
   "metadata": {},
   "source": [
    "### Setup vector DB (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d222640ccefee9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T09:57:35.205770Z",
     "start_time": "2025-01-29T09:57:34.686437Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.vectorstores import VectorStore, VectorStoreRetriever\n",
    "\n",
    "persist_directory = './db/rag_simple_exercise/'\n",
    "\n",
    "# Optionally remove the directory and all files in it recursively if it exists\n",
    "import shutil\n",
    "import os\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "\n",
    "vectordb: Chroma = Chroma(\n",
    "    collection_name=\"rag_simple_exercise\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory # Persist the database\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2bb279f787706",
   "metadata": {},
   "source": [
    "### Setup a text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bdfcbd420696d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T09:57:35.217676Z",
     "start_time": "2025-01-29T09:57:35.210335Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7chful029u8",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Let's take our splits and embed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4w6qz9f9uoh",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6w53pa2wq7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding_model.embed_query(sentence1)\n",
    "embedding2 = embedding_model.embed_query(sentence2)\n",
    "embedding3 = embedding_model.embed_query(sentence3)\n",
    "\n",
    "print(embedding1[:10])\n",
    "#print(len(embedding1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bo8hwfvy85o",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layy2fss68l",
   "metadata": {},
   "source": [
    "Embedding 1 and 2 should be similar (using NumPy's dot product to calculate similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2zd7rj6ykzm",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u484zkiq60j",
   "metadata": {},
   "source": [
    "But Embedding 3 should differ more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1gcfq7v3kg",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wp8r9odvcoc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1kqr06l0ua",
   "metadata": {},
   "source": [
    "## Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5jf8t6cqqa",
   "metadata": {},
   "source": [
    "### PDFs\n",
    "\n",
    "PDFs can be loaded in a number of different ways, but the easiest is by using the `PyPDFLoader` class. PDFs can be loaded from a local file or a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mp94ke06qh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#loader = PyPDFLoader(\"some_local_file.pdf\")\n",
    "loader = PyPDFLoader(\"https://data.riksdagen.se/fil/61B7540B-EEDD-4922-B61B-FC0A9F3AE4E2\") # 2024/25:263 AI, annan ny teknik och de m√§nskliga r√§ttigheterna\n",
    "#loader = PyPDFLoader(\"https://data.riksdagen.se/fil/0D43150B-5B31-43A4-89CD-4FE0478EC6C7\") # 2024/25:263 AI, annan ny teknik och de m√§nskliga r√§ttigheterna (svar)\n",
    "pdf_pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9tjjbaju",
   "metadata": {},
   "source": [
    "**Each page** is a `Document`.\n",
    "\n",
    "A `Document` contains text (`page_content`) and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8soqk4yp",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sml6cc7g0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf_pages[0]\n",
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9grozm3yaic",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4v654b57xo2",
   "metadata": {},
   "source": [
    "### Web Page\n",
    "\n",
    "There are a number of different ways of loading data from the web, but the easiest is by using the `WebBaseLoader` class, which uses the parser BeautifulSoup under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emyudyeh8i",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "page_url = \"https://world.hey.com/dhh/open-source-royalty-and-mad-kings-a8f79d16\"\n",
    "loader = WebBaseLoader(page_url)\n",
    "# loader = WebBaseLoader(page_url, header_template={\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0vfeon1d5l1",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4mqk921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(web_docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee64e2b269762e9",
   "metadata": {},
   "source": [
    "### Ingest - load, split and add to vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1319121a51e71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T09:57:35.225128Z",
     "start_time": "2025-01-29T09:57:35.222787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Documents to load\n",
    "\n",
    "# Load\n",
    "loader = PyPDFLoader(\"https://data.riksdagen.se/fil/61B7540B-EEDD-4922-B61B-FC0A9F3AE4E2\")\n",
    "pages = loader.load()\n",
    "\n",
    "# Split\n",
    "doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Add to index\n",
    "print(f\"Adding document to index...\")\n",
    "vectordb.add_documents(documents=doc_splits)\n",
    "\n",
    "print(f\"Added document - {len(pages)} pages - {len(doc_splits)} splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1dc3a4f6d9fb17",
   "metadata": {},
   "source": [
    "## Setup query graph / pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83c677f583ec19",
   "metadata": {},
   "source": [
    "### Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f33014df9438d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import  List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class GraphState(MessagesState):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b3adb4e0bee5",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86188187c933bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a970e8996661772",
   "metadata": {},
   "source": [
    "#### Retrieval (Vector Store similarity search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370fb2ff04b4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalNode:\n",
    "    retriever: VectorStoreRetriever\n",
    "\n",
    "    def __init__(self):\n",
    "        self.retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---RETRIEVE---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        # Retrieval\n",
    "        documents = self.retriever.invoke(question)\n",
    "\n",
    "        print(f\"---RETRIEVED {len(documents)} DOCS---\")\n",
    "        #print(f\"{documents}\")\n",
    "\n",
    "        return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1476bcc382c5f",
   "metadata": {},
   "source": [
    "#### RAG Generation (LLM call with factual/grounded context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33440df4763d1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGNode:\n",
    "    system_template = \"\"\"You are an helpful assistant, expert in answering questions based on provided sources (snippets from documents) and citing the sources used to generate the answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
    "    ALWAYS respond in the SAME language as the original question.\n",
    "\n",
    "    ** Context (snippets from documents): **\n",
    "\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---GENERATE---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # RAG generation - setup context (i.e. relevant documents snippets)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "        # RAG generation - generate answer\n",
    "        answer = self.chain.invoke({\"question\": question, \"context\": context})\n",
    "        #print(f\"---GENERATE - ANSWER: \\n{answer}\")\n",
    "\n",
    "        return {\"documents\": documents, \"answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3947d8187653a",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04bc48f05ca2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Graph ####\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", RetrievalNode())  # retrieve\n",
    "workflow.add_node(\"generate\", RAGNode())  # generate\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e36939b57669f",
   "metadata": {},
   "source": [
    "## Use Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e461d67c05d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"Vad har sagts om m√§nskliga r√§ttigheter och artificiell intelligens (AI)?\"\n",
    "    #\"question\": \"Vilka √§r nobelpristagarna 2024?\" # Should result in \"Jag vet inte.\"\n",
    "}\n",
    "\n",
    "# Execute graph\n",
    "result = graph.invoke(inputs)\n",
    "\n",
    "print(f\"--- ANSWER: ---\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfc3123e5cd1d8",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "-----\n",
    "\n",
    "## Going even further - adding grading of retrieved documents for relevance (Corrective RAG)\n",
    "\n",
    "#### Look at **`simple-rag-agent-demo.ipynb`** for inspiration - and try to implement a similar setup here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
