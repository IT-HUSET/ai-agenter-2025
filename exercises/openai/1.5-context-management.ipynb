{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Context and Conversation ManagementThis notebook focuses on **context engineering** - managing information for AI applications.**Key Concepts:**- Token limits and budgeting- Conversation management strategies- Stateful conversations (automatic state)- CLAUDE.md pattern for project context- R&D: Reduce and Delegate**Why this matters:** Effective context management is critical for building robust AI applications.<a target=\"_blank\" href=\"https://githubtocolab.com/IT-HUSET/ai-agenter-2025/blob/main/exercises/openai/1.5-context-management.ipynb\">  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai~=2.1 python-dotenv~=1.0 tiktoken~=0.8 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom openai import OpenAI\nimport tiktoken\n\n# Check if running in Google Colab\ntry:\n    from google.colab import userdata\n    IN_COLAB = True\n    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n    print(\"âœ… Running in Google Colab - API key loaded from secrets\")\nexcept ImportError:\n    IN_COLAB = False\n    try:\n        from dotenv import load_dotenv, find_dotenv\n        load_dotenv(find_dotenv())\n        print(\"âœ… Running locally - API key loaded from .env file\")\n    except ImportError:\n        print(\"âš ï¸ python-dotenv not installed\")\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    print(\"âŒ OPENAI_API_KEY not found!\")\n    if IN_COLAB:\n        print(\"   â†’ Click the key icon (ðŸ”‘) in the left sidebar and add 'OPENAI_API_KEY'\")\nelse:\n    print(\"âœ… Setup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Token Limits\n",
    "\n",
    "Before we manage context, we need to understand what we're managing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, model: str = \"gpt-5\") -> int:\n",
    "    \"\"\"Count tokens in a text string\"\"\"\n",
    "    # Use cl100k_base encoding for GPT-4 and GPT-5 models\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Test with different texts\n",
    "texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Supercalifragilisticexpialidocious\",\n",
    "    \"A\" * 100,\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    tokens = count_tokens(text)\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Length: {len(text)} chars, {tokens} tokens\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Context Windows\n",
    "\n",
    "Different models have different context window sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context window sizes (as of 2025)\n",
    "context_windows = {\n",
    "    \"gpt-5\": 200_000,\n",
    "    \"gpt-5-mini\": 200_000,\n",
    "    \"gpt-5\": 128_000,\n",
    "    \"gpt-5-mini\": 128_000,\n",
    "    \"o3-mini\": 200_000,\n",
    "}\n",
    "\n",
    "print(\"Model Context Windows:\")\n",
    "for model, window in context_windows.items():\n",
    "    print(f\"  {model}: {window:,} tokens\")\n",
    "\n",
    "# Calculate how many pages of text fit\n",
    "avg_tokens_per_page = 500  # Rough estimate\n",
    "print(f\"\\nRoughly {context_windows['gpt-5'] // avg_tokens_per_page:,} pages in GPT-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 1: Token Budgeting\n",
    "\n",
    "**Task:** You have a 128K token context window. Budget tokens for:\n",
    "- System instructions\n",
    "- Conversation history\n",
    "- Retrieved documents (RAG)\n",
    "- Response generation\n",
    "\n",
    "Calculate how many messages and documents you can fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "CONTEXT_WINDOW = 128_000\n",
    "\n",
    "# Estimate token allocations\n",
    "system_instructions = 500  # tokens\n",
    "max_response = 4_000  # tokens\n",
    "doc_size = 1_000  # tokens per document\n",
    "message_size = 100  # tokens per message\n",
    "\n",
    "# TODO: Calculate:\n",
    "# - How many documents can you retrieve?\n",
    "# - How many conversation turns can you keep?\n",
    "# - What's your safety margin?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Conversation Context Management\n",
    "\n",
    "Managing conversation history is critical for agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowContext:\n",
    "    \"\"\"Keep only the N most recent messages\"\"\"\n",
    "    \n",
    "    def __init__(self, max_messages: int = 10):\n",
    "        self.max_messages = max_messages\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "        \n",
    "        # Keep only recent messages\n",
    "        if len(self.messages) > self.max_messages:\n",
    "            self.messages = self.messages[-self.max_messages:]\n",
    "    \n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "    \n",
    "    def get_token_count(self):\n",
    "        text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.messages])\n",
    "        return count_tokens(text)\n",
    "\n",
    "# Test it\n",
    "window = SlidingWindowContext(max_messages=5)\n",
    "\n",
    "for i in range(10):\n",
    "    window.add_message(\"user\", f\"Question {i}\")\n",
    "    window.add_message(\"assistant\", f\"Answer to question {i}\")\n",
    "\n",
    "print(f\"Messages kept: {len(window.get_messages())}\")\n",
    "print(f\"Total tokens: {window.get_token_count()}\")\n",
    "print(\"\\nMessages:\")\n",
    "for msg in window.get_messages():\n",
    "    print(f\"  {msg['role']}: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Summarization\n",
    "\n",
    "Instead of dropping old messages, summarize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizingContext:\n",
    "    \"\"\"Summarize old messages when window gets full\"\"\"\n",
    "    \n",
    "    def __init__(self, max_messages: int = 10, summarize_threshold: int = 8):\n",
    "        self.max_messages = max_messages\n",
    "        self.summarize_threshold = summarize_threshold\n",
    "        self.messages = []\n",
    "        self.summary = \"\"\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "        \n",
    "        # Trigger summarization when threshold is reached\n",
    "        if len(self.messages) >= self.summarize_threshold:\n",
    "            self._summarize_old_messages()\n",
    "    \n",
    "    def _summarize_old_messages(self):\n",
    "        \"\"\"Summarize oldest half of messages\"\"\"\n",
    "        split_point = len(self.messages) // 2\n",
    "        old_messages = self.messages[:split_point]\n",
    "        \n",
    "        # Create summary prompt\n",
    "        conversation = \"\\n\".join(\n",
    "            [f\"{m['role']}: {m['content']}\" for m in old_messages]\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"Summarize this conversation in 2-3 sentences. \n",
    "Focus on key topics, decisions, and context needed for future messages.\n",
    "\n",
    "{conversation}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        \n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5-mini\",\n",
    "            input=prompt,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Update summary and keep recent messages\n",
    "        new_summary = response.output_text\n",
    "        self.summary = f\"{self.summary}\\n\\n{new_summary}\" if self.summary else new_summary\n",
    "        self.messages = self.messages[split_point:]\n",
    "        \n",
    "        print(f\"ðŸ“ Summarized {split_point} messages\")\n",
    "    \n",
    "    def get_full_context(self):\n",
    "        \"\"\"Get summary + recent messages\"\"\"\n",
    "        context = \"\"\n",
    "        if self.summary:\n",
    "            context = f\"Previous conversation summary:\\n{self.summary}\\n\\n\"\n",
    "        \n",
    "        recent = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.messages])\n",
    "        return context + \"Recent conversation:\\n\" + recent\n",
    "\n",
    "# Test it\n",
    "summarizing = SummarizingContext(max_messages=10, summarize_threshold=6)\n",
    "\n",
    "topics = [\"Python\", \"Data structures\", \"Algorithms\", \"Machine learning\", \n",
    "          \"Neural networks\", \"Transformers\", \"LLMs\", \"Agents\"]\n",
    "\n",
    "for topic in topics:\n",
    "    summarizing.add_message(\"user\", f\"Tell me about {topic}\")\n",
    "    summarizing.add_message(\"assistant\", f\"Here's information about {topic}...\")\n",
    "\n",
    "print(\"\\nFull context:\")\n",
    "print(summarizing.get_full_context())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Token-Based Budgeting\n",
    "\n",
    "The most precise approach: manage based on actual token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenBudgetContext:\n",
    "    \"\"\"Manage context based on token budget\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 4000):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "        self._trim_to_budget()\n",
    "    \n",
    "    def _trim_to_budget(self):\n",
    "        \"\"\"Remove oldest messages until within budget\"\"\"\n",
    "        while self.get_token_count() > self.max_tokens and len(self.messages) > 1:\n",
    "            # Remove oldest message\n",
    "            removed = self.messages.pop(0)\n",
    "            print(f\"ðŸ—‘ï¸ Removed message: {removed['role']}: {removed['content'][:50]}...\")\n",
    "    \n",
    "    def get_token_count(self):\n",
    "        text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.messages])\n",
    "        return count_tokens(text)\n",
    "    \n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "\n",
    "# Test with long messages\n",
    "token_budget = TokenBudgetContext(max_tokens=500)\n",
    "\n",
    "for i in range(5):\n",
    "    long_text = f\"This is message {i}. \" * 50  # ~100-150 tokens each\n",
    "    token_budget.add_message(\"user\", long_text)\n",
    "\n",
    "print(f\"\\nFinal message count: {len(token_budget.get_messages())}\")\n",
    "print(f\"Total tokens: {token_budget.get_token_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 2: Hybrid Context Manager\n",
    "\n",
    "**Task:** Combine all three strategies:\n",
    "1. Use token budgeting as primary constraint\n",
    "2. Summarize when you hit 80% of budget\n",
    "3. Keep last N messages regardless (sliding window minimum)\n",
    "\n",
    "**Bonus:** Add message priority (mark important messages to always keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "class HybridContextManager:\n",
    "    def __init__(self, max_tokens: int = 4000, min_messages: int = 5):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.min_messages = min_messages\n",
    "        self.messages = []\n",
    "        self.summary = \"\"\n",
    "    \n",
    "    def add_message(self, role: str, content: str, priority: bool = False):\n",
    "        # TODO: Implement hybrid strategy\n",
    "        pass\n",
    "    \n",
    "    def _should_summarize(self) -> bool:\n",
    "        # TODO: Check if at 80% of budget\n",
    "        pass\n",
    "    \n",
    "    def _summarize(self):\n",
    "        # TODO: Summarize non-priority messages\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The CLAUDE.md Pattern\n",
    "\n",
    "**Context engineering** is about structuring your entire project for AI understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example CLAUDE.md Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_md_template = \"\"\"\n",
    "# Project Context\n",
    "\n",
    "## Project Description\n",
    "This is a task management API built with FastAPI. It provides CRUD operations \n",
    "for tasks, users, and projects with role-based access control.\n",
    "\n",
    "## Architecture\n",
    "- **Backend**: FastAPI (Python 3.11+)\n",
    "- **Database**: PostgreSQL via SQLAlchemy ORM\n",
    "- **Auth**: JWT tokens with refresh mechanism\n",
    "- **Testing**: pytest with async support\n",
    "- **Deployment**: Docker + Kubernetes\n",
    "\n",
    "## Key Files\n",
    "- `app/main.py` - FastAPI application entry point\n",
    "- `app/models/` - SQLAlchemy models\n",
    "- `app/routers/` - API endpoints\n",
    "- `app/services/` - Business logic\n",
    "- `tests/` - Test suite\n",
    "\n",
    "## Development Guidelines\n",
    "1. **Code Style**: Follow PEP 8, use black for formatting\n",
    "2. **Testing**: Write tests for all endpoints (>80% coverage)\n",
    "3. **Type Hints**: Use type hints for all functions\n",
    "4. **Async**: Use async/await for all I/O operations\n",
    "5. **Error Handling**: Use HTTPException with appropriate status codes\n",
    "\n",
    "## Common Tasks\n",
    "\n",
    "### Adding a New Endpoint\n",
    "```python\n",
    "# 1. Define Pydantic schema in app/schemas/\n",
    "class TaskCreate(BaseModel):\n",
    "    title: str\n",
    "    description: str | None = None\n",
    "\n",
    "# 2. Add route in app/routers/\n",
    "@router.post(\"/tasks/\", response_model=Task)\n",
    "async def create_task(task: TaskCreate, db: AsyncSession = Depends(get_db)):\n",
    "    return await task_service.create(db, task)\n",
    "\n",
    "# 3. Write tests in tests/\n",
    "async def test_create_task(client):\n",
    "    response = await client.post(\"/tasks/\", json={\"title\": \"Test\"})\n",
    "    assert response.status_code == 200\n",
    "```\n",
    "\n",
    "## Constraints & Guardrails\n",
    "- âŒ Never commit secrets or API keys\n",
    "- âŒ Don't modify the database schema without migrations\n",
    "- âŒ Don't use sync I/O in async functions\n",
    "- âœ… Always validate input with Pydantic\n",
    "- âœ… Use dependency injection for database sessions\n",
    "- âœ… Log all errors with context\n",
    "\n",
    "## Running the Project\n",
    "```bash\n",
    "# Development\n",
    "uv run uvicorn app.main:app --reload\n",
    "\n",
    "# Tests\n",
    "uv run pytest\n",
    "\n",
    "# Database migrations\n",
    "uv run alembic upgrade head\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "print(claude_md_template)\n",
    "print(f\"\\nContext size: {count_tokens(claude_md_template)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Context in API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use CLAUDE.md context for a coding task\n",
    "task = \"\"\"Add a new endpoint to assign a task to a user. \n",
    "The endpoint should:\n",
    "- Accept task_id and user_id\n",
    "- Verify the user has permission\n",
    "- Update the task's assigned_to field\n",
    "- Return the updated task\n",
    "\"\"\"\n",
    "\n",
    "# With context, the model understands project structure\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    instructions=claude_md_template,  # Project context\n",
    "    input=task,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"Generated code:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 3: Create Your CLAUDE.md\n",
    "\n",
    "**Task:** Write a CLAUDE.md for a project you're working on (or make one up).\n",
    "\n",
    "**Requirements:**\n",
    "- Include all sections from the template\n",
    "- Add at least 2 code examples\n",
    "- List 5+ constraints/guardrails\n",
    "- Keep under 2000 tokens\n",
    "\n",
    "**Test it:** Ask the model to generate code using your context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "your_claude_md = \"\"\"\n",
    "# TODO: Write your project context\n",
    "\"\"\"\n",
    "\n",
    "# Test it\n",
    "print(f\"Token count: {count_tokens(your_claude_md)}\")\n",
    "\n",
    "# Ask the model to do something\n",
    "# response = client.responses.create(\n",
    "#     model=\"gpt-5-mini\",\n",
    "#     instructions=your_claude_md,\n",
    "#     input=\"Generate a README.md for this project\"\n",
    "# )\n",
    "# print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: R&D - Reduce and Delegate\n",
    "\n",
    "Two principles for managing context effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce: Be Specific and Focused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: Too much irrelevant context\n",
    "bad_context = \"\"\"\n",
    "I have a Python project. It uses FastAPI. And SQLAlchemy. We also use pytest.\n",
    "The team likes to use async. We deploy on AWS. Sometimes we use Docker.\n",
    "Our database is PostgreSQL but we're thinking of switching to MongoDB.\n",
    "We have 10 developers. The project started in 2023...\n",
    "(continues for 2000 more tokens)\n",
    "\"\"\"\n",
    "\n",
    "# GOOD: Focused, relevant context\n",
    "good_context = \"\"\"\n",
    "FastAPI + SQLAlchemy (async) + PostgreSQL. \n",
    "Task: Add new endpoint for task assignment.\n",
    "Pattern: See app/routers/tasks.py for existing endpoints.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Bad context:\", count_tokens(bad_context), \"tokens\")\n",
    "print(\"Good context:\", count_tokens(good_context), \"tokens\")\n",
    "print(f\"\\nSavings: {count_tokens(bad_context) - count_tokens(good_context)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delegate: Use Multiple Focused Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of one huge context, use separate focused sessions\n",
    "\n",
    "# Session 1: Design the API\n",
    "design_task = \"Design an endpoint to assign tasks to users. Return the API spec.\"\n",
    "\n",
    "design_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=design_task,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "api_spec = design_response.output_text\n",
    "print(\"API Spec:\")\n",
    "print(api_spec)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Session 2: Implement based on spec (separate context)\n",
    "impl_task = f\"\"\"Implement this API spec using FastAPI:\n",
    "\n",
    "{api_spec}\n",
    "\n",
    "Follow FastAPI best practices. Use async. Include error handling.\n",
    "\"\"\"\n",
    "\n",
    "impl_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=impl_task,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"Implementation:\")\n",
    "print(impl_response.output_text[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Stateful Conversations\n",
    "\n",
    "The Responses API provides **automatic state management** via `previous_response_id`.\n",
    "\n",
    "**Benefits:**\n",
    "- No manual message tracking\n",
    "- Automatic context window management\n",
    "- Simpler code\n",
    "- Built-in optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic vs Manual State Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Completions: Manual state management\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are helpful\"}]\n",
    "\n",
    "# Turn 1\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's 2+2?\"})\n",
    "response = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "\n",
    "# Turn 2\n",
    "messages.append({\"role\": \"user\", \"content\": \"Multiply that by 3\"})\n",
    "response = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
    "\n",
    "print(\"Chat Completions result:\", response.choices[0].message.content)\n",
    "print(f\"Messages tracked: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responses API: Automatic state management\n",
    "# Turn 1\n",
    "response_1 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"What's 2+2?\"\n",
    ")\n",
    "\n",
    "# Turn 2 - just reference previous!\n",
    "response_2 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Multiply that by 3\",\n",
    "    previous_response_id=response_1.id  # âœ¨ Automatic!\n",
    ")\n",
    "\n",
    "print(\"Responses API result:\", response_2.output_text)\n",
    "print(\"\\nâœ… No manual tracking needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining with Context Management Strategies\n",
    "\n",
    "You can combine stateful conversations with the strategies from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulContextManager:\n",
    "    \"\"\"Combines automatic state with token budgeting\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 8000, model: str = \"gpt-4o\"):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.model = model\n",
    "        self.last_response_id = None\n",
    "    \n",
    "    def send(self, message: str) -> str:\n",
    "        \"\"\"Send message with automatic state\"\"\"\n",
    "        response = client.responses.create(\n",
    "            model=self.model,\n",
    "            input=message,\n",
    "            previous_response_id=self.last_response_id\n",
    "        )\n",
    "        \n",
    "        self.last_response_id = response.id\n",
    "        return response.output_text\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Start new conversation\"\"\"\n",
    "        self.last_response_id = None\n",
    "\n",
    "# Test it\n",
    "manager = StatefulContextManager()\n",
    "print(manager.send(\"What's the capital of France?\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(manager.send(\"What's its population?\"))  # Knows context!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Which Approach\n",
    "\n",
    "| Scenario | Use |\n",
    "|----------|-----|\n",
    "| **New projects** | Responses API (automatic) |\n",
    "| **Need fine control** | Chat Completions (manual) |\n",
    "| **Building agents** | Responses API |\n",
    "| **Existing codebase** | Chat Completions |\n",
    "| **Simple conversations** | Responses API |\n",
    "| **Complex state logic** | Chat Completions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 4: Reduce and Delegate in Practice\n",
    "\n",
    "**Task:** You need to add a complete feature: user authentication with email/password.\n",
    "\n",
    "Instead of one huge prompt, break it into focused sessions:\n",
    "1. Design the auth flow\n",
    "2. Design the database schema\n",
    "3. Implement password hashing\n",
    "4. Implement login endpoint\n",
    "5. Implement token refresh\n",
    "6. Write tests\n",
    "\n",
    "**Measure:** Compare total tokens used vs. putting everything in one prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def multi_session_approach():\n",
    "    \"\"\"Break into focused sessions\"\"\"\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Session 1: Design\n",
    "    # Session 2: Schema\n",
    "    # Session 3: Hashing\n",
    "    # ...\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "def single_session_approach():\n",
    "    \"\"\"Everything in one huge prompt\"\"\"\n",
    "    massive_prompt = \"\"\"TODO: Everything at once\"\"\"\n",
    "    return count_tokens(massive_prompt)\n",
    "\n",
    "# Compare\n",
    "# multi_tokens = multi_session_approach()\n",
    "# single_tokens = single_session_approach()\n",
    "# print(f\"Multi-session: {multi_tokens} tokens\")\n",
    "# print(f\"Single-session: {single_tokens} tokens\")\n",
    "# print(f\"Difference: {single_tokens - multi_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… **Token budgeting**: Understanding and managing context limits  \n",
    "âœ… **Conversation management**: Sliding window, summarization, token budgets  \n",
    "âœ… **Stateful conversations**: Automatic state with `previous_response_id`  \n",
    "âœ… **CLAUDE.md pattern**: Structuring project context  \n",
    "âœ… **R&D principle**: Reduce and Delegate for efficiency  \n",
    "\n",
    "**Key Takeaways:**\n",
    "- Token limits are real constraints - plan accordingly\n",
    "- Use Responses API for automatic state management\n",
    "- Different strategies for different needs (sliding window vs. summarization)\n",
    "- CLAUDE.md provides consistent context for AI coding agents\n",
    "- Break complex tasks into focused sessions (Delegate)\n",
    "- Only include relevant information (Reduce)\n",
    "\n",
    "**Next Steps:**\n",
    "- Notebook 1.6: Agentic Applications (combine everything)\n",
    "- Apply these patterns in your LangGraph agents\n",
    "- Create CLAUDE.md for your projects\n",
    "\n",
    "**Resources:**\n",
    "- [OpenAI Responses API Docs](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [Token Counting with tiktoken](https://github.com/openai/tiktoken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}