{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 OpenAI API - Advanced Features\n",
    "\n",
    "This notebook covers **advanced OpenAI API features** for building sophisticated AI agents:\n",
    "\n",
    "1. **Function Calling & Tool Role**: Enable AI to call external tools\n",
    "2. **Reasoning Modes**: Extended thinking for complex problems\n",
    "3. **Model Selection**: Choose the right GPT-5 model for your use case\n",
    "4. **Advanced Role Patterns**: Developer vs user role hierarchy\n",
    "\n",
    "**Prerequisites:** Complete notebook 1.1 (OpenAI API Basics) first.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/IT-HUSET/ai-agents-course-2025/blob/main/exercises/1.2-openai-api-advanced.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai~=1.60 python-dotenv~=1.0 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"âœ… OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Function Calling & Tool Role\n",
    "\n",
    "Function calling enables AI models to interact with external tools and APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tool Role - For Function Calling\n",
    "\n",
    "The **`tool`** role is used when the model calls a function and you need to provide the result back.\n",
    "\n",
    "**Function Calling Flow:**\n",
    "```\n",
    "1. User asks a question\n",
    "2. Model decides to call a tool â†’ returns tool_call\n",
    "3. You execute the function\n",
    "4. You send result back with role: \"tool\" â† This is the tool role\n",
    "5. Model uses result to answer user\n",
    "```\n",
    "\n",
    "**Chat Completions API Roles:**\n",
    "\n",
    "| Role | Purpose | Who Creates It |\n",
    "|------|---------|----------------|\n",
    "| `system` | Persistent behavior instructions | Developer |\n",
    "| `user` | User's queries | User/Developer |\n",
    "| `assistant` | Model's responses | Model |\n",
    "| `tool` | Function call results | Developer |\n",
    "\n",
    "**Important:** You'll see the `tool` role in action in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete function calling flow with tool role\n",
    "import json\n",
    "\n",
    "# Define a simple function\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Fake weather API\"\"\"\n",
    "    return {\n",
    "        \"city\": city,\n",
    "        \"temperature\": 22,\n",
    "        \"condition\": \"sunny\",\n",
    "        \"humidity\": 65\n",
    "    }\n",
    "\n",
    "# Define tool for the model\n",
    "weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather for a city\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city name\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 1: User asks a question\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in Stockholm?\"}\n",
    "]\n",
    "\n",
    "print(\"Step 1: User asks question\")\n",
    "print(f\"Messages: {len(messages)}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Model decides to call the tool\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages,\n",
    "    tools=[weather_tool],\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "message = response.choices[0].message\n",
    "print(\"Step 2: Model wants to call tool\")\n",
    "print(f\"Tool calls: {message.tool_calls}\")\n",
    "print()\n",
    "\n",
    "# Add assistant's tool call to history\n",
    "messages.append(message)\n",
    "\n",
    "# Step 3 & 4: Execute function and add result with 'tool' role\n",
    "if message.tool_calls:\n",
    "    tool_call = message.tool_calls[0]\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    \n",
    "    # Execute the function\n",
    "    result = get_weather(**function_args)\n",
    "    \n",
    "    print(\"Step 3: Execute function\")\n",
    "    print(f\"Function: {tool_call.function.name}\")\n",
    "    print(f\"Arguments: {function_args}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print()\n",
    "    \n",
    "    # Add function result with 'tool' role\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",  # â† THE TOOL ROLE!\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"content\": json.dumps(result)\n",
    "    })\n",
    "    \n",
    "    print(\"Step 4: Add result with 'tool' role\")\n",
    "    print(f\"Messages now: {len(messages)}\")\n",
    "    print()\n",
    "\n",
    "# Step 5: Model generates final answer using the tool result\n",
    "final_response = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"Step 5: Model's final answer:\")\n",
    "print(final_response.choices[0].message.content)\n",
    "print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Final message history:\")\n",
    "for i, msg in enumerate(messages):\n",
    "    role = msg.get('role') if isinstance(msg, dict) else msg.role\n",
    "    print(f\"{i+1}. {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Preambles (GPT-5 Feature)\n",
    "\n",
    "**Preambles** are brief explanations GPT-5 generates *before* calling a tool, explaining its intent.\n",
    "\n",
    "**Benefits:**\n",
    "- ðŸ” **Transparency**: See why the model chose a tool\n",
    "- ðŸ› **Debuggability**: Easier to understand failures\n",
    "- âœ… **Accuracy**: Improves tool-calling success rate\n",
    "- ðŸŽ¯ **User experience**: Users see the reasoning\n",
    "\n",
    "**How to enable:** Add an instruction asking the model to explain before calling tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tool calling WITH preambles\n",
    "messages_with_preamble = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Before calling any tool, briefly explain why you're calling it.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather in Stockholm and Paris? Compare them.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Tool calling WITH preambles:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages_with_preamble,\n",
    "    tools=[weather_tool],\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "message = response.choices[0].message\n",
    "\n",
    "# The model will explain its reasoning before calling tools\n",
    "if message.content:\n",
    "    print(\"\\nðŸ§  Preamble (model's explanation):\")\n",
    "    print(message.content)\n",
    "\n",
    "if message.tool_calls:\n",
    "    print(f\"\\nðŸ› ï¸  Tool calls: {len(message.tool_calls)}\")\n",
    "    for tool_call in message.tool_calls:\n",
    "        print(f\"  - {tool_call.function.name}({tool_call.function.arguments})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preamble Example Output:**\n",
    "\n",
    "```\n",
    "ðŸ§  Preamble (model's explanation):\n",
    "I'll check the weather for both Stockholm and Paris so I can compare them.\n",
    "\n",
    "ðŸ› ï¸ Tool calls: 2\n",
    "  - get_weather({\"city\": \"Stockholm\"})\n",
    "  - get_weather({\"city\": \"Paris\"})\n",
    "```\n",
    "\n",
    "This makes the model's reasoning visible and improves user trust!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 1: Build a Multi-Tool Agent\n",
    "\n",
    "**Task:** Create an agent with multiple tools:\n",
    "1. `get_weather(city)` - get weather\n",
    "2. `get_time(city)` - get current time\n",
    "3. `convert_currency(amount, from_currency, to_currency)` - currency conversion\n",
    "\n",
    "Test it with: \"What's the weather in Tokyo, what time is it there, and how much is 100 USD in JPY?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Define your functions\n",
    "def get_time(city: str) -> dict:\n",
    "    # TODO: Implement (can be fake data for this exercise)\n",
    "    pass\n",
    "\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> dict:\n",
    "    # TODO: Implement (can be fake data for this exercise)\n",
    "    pass\n",
    "\n",
    "# Define your tools\n",
    "# TODO: Create tool definitions for all three functions\n",
    "\n",
    "# Test your multi-tool agent\n",
    "# TODO: Implement the agent loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Reasoning Modes\n",
    "\n",
    "GPT-5 models support **reasoning modes** - extended thinking for complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Reasoning Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "complex_problem = \"\"\"A farmer has chickens and rabbits. \n",
    "There are 35 heads and 94 legs total. \n",
    "How many chickens and how many rabbits are there?\n",
    "\"\"\"\n",
    "\n",
    "# Regular mode\n",
    "start = time.time()\n",
    "response_normal = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=complex_problem\n",
    ")\n",
    "time_normal = time.time() - start\n",
    "\n",
    "print(\"WITHOUT Reasoning Mode:\")\n",
    "print(f\"Time: {time_normal:.2f}s\")\n",
    "print(response_normal.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Reasoning Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5 with high reasoning effort\n",
    "start = time.time()\n",
    "response_reasoning = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning_effort=\"high\",  # GPT-5-mini and GPT-5 support reasoning\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": complex_problem}\n",
    "    ]\n",
    ")\n",
    "time_reasoning = time.time() - start\n",
    "\n",
    "print(\"WITH Reasoning Mode (GPT-5 with high reasoning):\")\n",
    "print(f\"Time: {time_reasoning:.2f}s\")\n",
    "\n",
    "# Check if reasoning tokens are available\n",
    "if hasattr(response_reasoning.choices[0].message, 'reasoning'):\n",
    "    print(\"\\nðŸ§  Internal Reasoning (preview):\")\n",
    "    reasoning = response_reasoning.choices[0].message.reasoning or \"[Hidden]\"\n",
    "    print(reasoning[:500] + \"...\" if len(reasoning) > 500 else reasoning)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Final Answer:\")\n",
    "print(response_reasoning.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Reasoning Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine token usage\n",
    "print(\"Token Usage Analysis:\")\n",
    "print(f\"Prompt tokens: {response_reasoning.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response_reasoning.usage.completion_tokens}\")\n",
    "\n",
    "if hasattr(response_reasoning.usage, 'reasoning_tokens'):\n",
    "    # GPT-5 includes reasoning tokens when using reasoning_effort\n",
    "    print(f\"Reasoning tokens: {response_reasoning.usage.reasoning_tokens}\")\n",
    "    print(f\"\\nðŸ’¡ Model spent {response_reasoning.usage.reasoning_tokens} tokens 'thinking' internally\")\n",
    "\n",
    "print(f\"Total tokens: {response_reasoning.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Reasoning Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex coding problem\n",
    "coding_problem = \"\"\"Find all bugs in this Python code and explain how to fix them:\n",
    "\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for i in range(len(numbers)):\n",
    "        total = total + numbers[i]\n",
    "    return total / len(numbers)\n",
    "\n",
    "result = calculate_average([1, 2, 3, 4, 5])\n",
    "print(f\"Average: {result}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"GPT-5-mini (fast):\")\n",
    "response_fast = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=coding_problem\n",
    ")\n",
    "print(response_fast.output_text)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"GPT-5 with high reasoning (reasoning):\")\n",
    "response_smart = client.chat.completions.create(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning_effort=\"high\",\n",
    "    messages=[{\"role\": \"user\", \"content\": coding_problem}]\n",
    ")\n",
    "print(response_smart.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise 2: Test Reasoning Capabilities\n",
    "\n",
    "**Task:** Compare GPT-5-mini vs GPT-5 on these problems:\n",
    "1. A logic puzzle of your choice\n",
    "2. A multi-step math problem\n",
    "3. A code optimization challenge\n",
    "\n",
    "**Analyze:**\n",
    "- Which model gives more accurate answers?\n",
    "- How much longer does reasoning take?\n",
    "- When is the extra cost worth it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "logic_puzzle = \"\"\"TODO: Write a challenging logic puzzle\"\"\"\n",
    "\n",
    "def compare_models(problem: str):\n",
    "    \"\"\"Compare GPT-5-mini vs GPT-5 with high reasoning on a problem\"\"\"\n",
    "    # TODO: Implement comparison\n",
    "    pass\n",
    "\n",
    "# Test your puzzles\n",
    "# compare_models(logic_puzzle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Model Selection & Pricing\n",
    "\n",
    "**GPT-5** (released August 2024) is the latest model family:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Model Selection Guide\n",
    "\n",
    "| Feature | GPT-4o | GPT-5 | GPT-5-mini | GPT-5-nano |\n",
    "|---------|--------|-------|------------|------------|\n",
    "| **Context Window** | 128K | 400K | 400K | 400K |\n",
    "| **Max Output** | 16,384 | 128K | 128K | 128K |\n",
    "| **Knowledge Cutoff** | Oct 2023 | Sep 2024 | May 2024 | May 2024 |\n",
    "| **Reasoning Levels** | âŒ None | ðŸ¤”ðŸ¤”ðŸ¤”ðŸ¤” (4/4) | ðŸ¤”ðŸ¤”ðŸ¤” (3/4) | ðŸ¤”ðŸ¤” (2/4) |\n",
    "| **Speed** | âš¡âš¡âš¡ | âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡âš¡ |\n",
    "| **Input ($/1M)** | $2.50 | $1.25 | $0.25 | $0.05 |\n",
    "| **Cached Input** | $1.25 | $0.13 | $0.03 | $0.01 |\n",
    "| **Output ($/1M)** | $10.00 | $10.00 | $2.00 | $0.40 |\n",
    "| **Vision** | âœ… | âœ… | âœ… | âœ… |\n",
    "| **Audio** | âœ… | âœ… | âœ… | âœ… |\n",
    "| **Reasoning Tokens** | âŒ | âœ… | âœ… | âœ… |\n",
    "| **Best For** | Legacy | Best quality | Balanced | Simple/cheap |\n",
    "\n",
    "**Key Insights:**\n",
    "- **GPT-5 is cheaper than GPT-4o** for input ($1.25 vs $2.50)\n",
    "- **All GPT-5 models** have 400K context (3x larger than GPT-4o)\n",
    "- **Reasoning tokens** are included when using reasoning_effort\n",
    "- **Cached inputs** are ~90% cheaper (great for repeated prompts)\n",
    "\n",
    "**Reasoning Levels Explained:**\n",
    "- **GPT-5**: minimal, low, medium, high (all 4)\n",
    "- **GPT-5-mini**: minimal, low, medium (no high)\n",
    "- **GPT-5-nano**: minimal, low (basic reasoning only)\n",
    "\n",
    "**Decision Tree:**\n",
    "```\n",
    "What's your use case?\n",
    "\n",
    "Complex reasoning (math, code debugging, strategy)?\n",
    "â”œâ”€ Need highest quality? â†’ GPT-5 (reasoning_effort=\"high\")\n",
    "â””â”€ Budget conscious? â†’ GPT-5-mini (reasoning_effort=\"medium\")\n",
    "\n",
    "Standard tasks (chat, content, analysis)?\n",
    "â”œâ”€ Need best quality? â†’ GPT-5\n",
    "â”œâ”€ Balanced quality/cost? â†’ GPT-5-mini\n",
    "â””â”€ Simple/repetitive? â†’ GPT-5-nano\n",
    "\n",
    "Cost is primary concern?\n",
    "â”œâ”€ Simple tasks â†’ GPT-5-nano ($0.45/1M total)\n",
    "â””â”€ Complex tasks â†’ GPT-5-mini ($2.25/1M total)\n",
    "```\n",
    "\n",
    "**Cost Comparison (1M input + 1M output):**\n",
    "```python\n",
    "# Without caching:\n",
    "GPT-5-nano:  $0.05 + $0.40  = $0.45   (10x cheaper than GPT-4o)\n",
    "GPT-5-mini:  $0.25 + $2.00  = $2.25   (4x cheaper than GPT-4o)\n",
    "GPT-5:       $1.25 + $10.00 = $11.25  (same output cost as GPT-4o)\n",
    "GPT-4o:      $2.50 + $10.00 = $12.50  (legacy)\n",
    "\n",
    "# With 90% cached input (common in agents):\n",
    "GPT-5-nano:  $0.01 + $0.40  = $0.41   (97% cheaper!)\n",
    "GPT-5-mini:  $0.03 + $2.00  = $2.03\n",
    "GPT-5:       $0.13 + $10.00 = $10.13\n",
    "```\n",
    "\n",
    "**When to use reasoning_effort:**\n",
    "- **Don't specify** (defaults to auto) - for most tasks\n",
    "- **\"low\"** - Light reasoning, faster\n",
    "- **\"medium\"** - Balanced (good default for complex tasks)\n",
    "- **\"high\"** - Maximum reasoning (GPT-5 only, for hardest problems)\n",
    "\n",
    "**âš ï¸ Note:** Higher reasoning effort increases reasoning tokens (hidden cost added to output tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Advanced Role Patterns\n",
    "\n",
    "Understanding the role hierarchy in different APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developer and User Roles in Responses API\n",
    "\n",
    "The Responses API introduces a **role hierarchy** different from Chat Completions:\n",
    "\n",
    "**Roles in Responses API:**\n",
    "- **`developer`**: Instructions from the application developer (highest priority)\n",
    "- **`user`**: Instructions/input from the end user (lower priority)\n",
    "- **`assistant`**: Model's responses\n",
    "\n",
    "**Priority:** `developer` > `user`\n",
    "\n",
    "The `instructions` parameter is shorthand for a `developer` role message. These are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using instructions parameter\n",
    "response1 = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    instructions=\"Talk like a pirate.\",\n",
    "    input=\"Are semicolons optional in JavaScript?\"\n",
    ")\n",
    "\n",
    "print(\"Method 1 - Using instructions parameter:\")\n",
    "print(response1.output_text)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Method 2: Using developer role in input array (equivalent)\n",
    "response2 = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Talk like a pirate.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Are semicolons optional in JavaScript?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Method 2 - Using developer role in input:\")\n",
    "print(response2.output_text)\n",
    "\n",
    "print(\"\\nðŸ’¡ Both methods produce similar results. Use `instructions` for simple cases, `developer` role for more complex message structures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Role Priority\n",
    "\n",
    "**In Chat Completions API:**\n",
    "- `system` role provides persistent instructions\n",
    "- All roles are treated equally in conversation flow\n",
    "\n",
    "**In Responses API:**\n",
    "- `developer` role has **higher priority** than `user` role\n",
    "- This ensures app-level instructions override user attempts to change behavior\n",
    "- Critical for building safe, reliable applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… **Function Calling**: How to use the `tool` role to integrate external functions  \n",
    "âœ… **Tool Preambles**: GPT-5 feature for transparent reasoning  \n",
    "âœ… **Reasoning Modes**: Extended thinking with `reasoning_effort` parameter  \n",
    "âœ… **Model Selection**: Complete guide to GPT-5, GPT-5-mini, and GPT-5-nano  \n",
    "âœ… **Role Hierarchy**: Developer vs user roles in Responses API\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Function calling enables AI to interact with external tools\n",
    "- Reasoning modes improve quality on complex problems (at higher cost)\n",
    "- Choose the right GPT-5 model based on your use case and budget\n",
    "- Understand role priorities when building production applications\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 1.3**: Structured outputs with JSON schemas\n",
    "- **Notebook 1.4**: Prompt engineering techniques\n",
    "- **Notebook 1.5**: Context management strategies\n",
    "- **Notebook 1.6**: Agentic applications\n",
    "\n",
    "**Resources:**\n",
    "- [Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [Reasoning Models Guide](https://platform.openai.com/docs/guides/reasoning)\n",
    "- [Model Pricing](https://openai.com/pricing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
