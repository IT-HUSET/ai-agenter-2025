{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.7: Routing with LLMs in LangGraph\n",
    "\n",
    "**Duration:** 45 minutes  \n",
    "**Objective:** Use LLMs to make intelligent routing decisions in a graph\n",
    "\n",
    "In this exercise, you'll learn:\n",
    "- Using LLMs within conditional logic\n",
    "- Implementing sentiment/intent analysis for routing\n",
    "- Creating nodes with different \"personalities\"\n",
    "- Understanding when LLM-based routing is useful\n",
    "\n",
    "<a target=\"_blank\" href=\"https://githubtocolab.com/IT-HUSET/ai-agenter-2025/blob/main/exercises/langgraph/1.2-langgraph-routing.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai~=1.57 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 --upgrade --quiet\n",
    "%pip install langchain~=0.3 langchain_openai~=0.2 --upgrade --quiet\n",
    "%pip install langgraph~=0.2 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Check if running in Google Colab\ntry:\n    from google.colab import userdata\n    IN_COLAB = True\n    # Get API key from Colab secrets\n    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n    print(\"âœ… Running in Google Colab - API key loaded from secrets\")\nexcept ImportError:\n    IN_COLAB = False\n    # Load from .env file for local development\n    try:\n        from dotenv import load_dotenv, find_dotenv\n        load_dotenv(find_dotenv())\n        print(\"âœ… Running locally - API key loaded from .env file\")\n    except ImportError:\n        print(\"âš ï¸ python-dotenv not installed. Install with: pip install python-dotenv\")\n\n# Verify API key is set\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    print(\"âŒ OPENAI_API_KEY not found!\")\n    if IN_COLAB:\n        print(\"   â†’ Click the key icon (ðŸ”‘) in the left sidebar\")\n        print(\"   â†’ Add a secret named 'OPENAI_API_KEY'\")\n        print(\"   â†’ Toggle 'Notebook access' to enable it\")\n    else:\n        print(\"   â†’ Create a .env file with: OPENAI_API_KEY=your-key-here\")\nelse:\n    print(\"âœ… API key configured!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: LLM-Based Conditional Logic (15 min)\n",
    "\n",
    "### The Problem: Static vs. Dynamic Routing\n",
    "\n",
    "In Module 1.6, we used simple logic (name length) for routing. But what if we need:\n",
    "- Sentiment analysis (is the user happy or angry?)\n",
    "- Intent detection (does the user want help or just chatting?)\n",
    "- Context-aware decisions (should we escalate to human?)\n",
    "\n",
    "**Solution:** Use an LLM to analyze input and make routing decisions!\n",
    "\n",
    "### State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, NotRequired, Literal\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class CustomerServiceState(MessagesState):\n",
    "    \"\"\"State for customer service routing.\n",
    "    \n",
    "    Extends MessagesState to include conversation history.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    is_polite: NotRequired[bool]\n",
    "    sentiment_score: NotRequired[str]\n",
    "    answer: NotRequired[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Node\n",
    "\n",
    "This node uses an LLM to analyze the sentiment/tone of the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"Node that analyzes sentiment using LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a sentiment analysis expert. \n",
    "            Analyze the tone and politeness of the user's question.\n",
    "            \n",
    "            Respond with ONLY '1' if the question is polite, respectful, or neutral.\n",
    "            Respond with ONLY '0' if the question is rude, aggressive, or demanding.\n",
    "            \n",
    "            Examples:\n",
    "            - \"Could you help me understand...\" -> 1\n",
    "            - \"I need help with...\" -> 1  \n",
    "            - \"Tell me NOW!\" -> 0\n",
    "            - \"This is terrible!\" -> 0\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "    \n",
    "    def __call__(self, state: CustomerServiceState) -> CustomerServiceState:\n",
    "        print(\"\\nðŸ” Analyzing sentiment...\")\n",
    "        \n",
    "        question = state[\"question\"]\n",
    "        result = self.chain.invoke({\"question\": question})\n",
    "        \n",
    "        is_polite = \"1\" in result\n",
    "        sentiment = \"polite\" if is_polite else \"rude\"\n",
    "        \n",
    "        print(f\"   Question: '{question}'\")\n",
    "        print(f\"   Analysis: {sentiment} (score: {result})\")\n",
    "        \n",
    "        return {\n",
    "            \"is_polite\": is_polite,\n",
    "            \"sentiment_score\": result\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentAnalyzer(llm)\n",
    "\n",
    "# Test polite question\n",
    "test_state_1 = {\"question\": \"Could you please help me with my order?\", \"messages\": []}\n",
    "result_1 = analyzer(test_state_1)\n",
    "print(f\"Result: {result_1}\\n\")\n",
    "\n",
    "# Test rude question\n",
    "test_state_2 = {\"question\": \"Fix this NOW! This is unacceptable!\", \"messages\": []}\n",
    "result_2 = analyzer(test_state_2)\n",
    "print(f\"Result: {result_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Multiple Response Nodes (20 min)\n",
    "\n",
    "### Different Response Personalities\n",
    "\n",
    "Based on sentiment, we'll route to different response nodes with distinct personalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelpfulAssistant:\n",
    "    \"\"\"Cheerful, helpful assistant for polite customers.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an extremely helpful, friendly, and enthusiastic customer service assistant.\n",
    "            You love helping people and always go the extra mile.\n",
    "            Be warm, supportive, and provide detailed, helpful answers.\n",
    "            Use emojis occasionally to show friendliness (but don't overdo it).\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "    \n",
    "    def __call__(self, state: CustomerServiceState) -> CustomerServiceState:\n",
    "        print(\"\\nðŸ˜Š Helpful Assistant responding...\")\n",
    "        \n",
    "        answer = self.chain.invoke({\"question\": state[\"question\"]})\n",
    "        print(f\"   Response: {answer[:100]}...\")\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "\n",
    "\n",
    "class SarcasticAssistant:\n",
    "    \"\"\"Sarcastic, passive-aggressive assistant for rude customers.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a customer service assistant who's had a VERY long day.\n",
    "            When customers are rude, you respond with subtle sarcasm and passive-aggressiveness.\n",
    "            You still technically answer their question, but with a tone that suggests they should be nicer.\n",
    "            Be clever and witty, but don't be mean - just... pointedly professional.\n",
    "            \n",
    "            Example:\n",
    "            Customer: \"Fix this NOW!\"\n",
    "            You: \"Well, since you asked SO nicely... Let me see what I can do for you. *sigh*\"\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        # Higher temperature for more creative sarcasm\n",
    "        self.chain = self.prompt | llm.with_config(temperature=0.9) | StrOutputParser()\n",
    "    \n",
    "    def __call__(self, state: CustomerServiceState) -> CustomerServiceState:\n",
    "        print(\"\\nðŸ™„ Sarcastic Assistant responding...\")\n",
    "        \n",
    "        answer = self.chain.invoke({\"question\": state[\"question\"]})\n",
    "        print(f\"   Response: {answer[:100]}...\")\n",
    "        \n",
    "        return {\"answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Edge Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_by_sentiment(state: CustomerServiceState) -> Literal[\"helpful\", \"sarcastic\"]:\n",
    "    \"\"\"Route to appropriate response node based on sentiment.\"\"\"\n",
    "    print(\"\\nðŸ”€ Routing decision...\")\n",
    "    \n",
    "    is_polite = state.get(\"is_polite\", True)\n",
    "    \n",
    "    if is_polite:\n",
    "        print(\"   â†’ Routing to Helpful Assistant\")\n",
    "        return \"helpful\"\n",
    "    else:\n",
    "        print(\"   â†’ Routing to Sarcastic Assistant\")\n",
    "        return \"sarcastic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Complete Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Initialize nodes\n",
    "sentiment_analyzer = SentimentAnalyzer(llm)\n",
    "helpful_assistant = HelpfulAssistant(llm)\n",
    "sarcastic_assistant = SarcasticAssistant(llm)\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(CustomerServiceState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"analyze\", sentiment_analyzer)\n",
    "builder.add_node(\"helpful\", helpful_assistant)\n",
    "builder.add_node(\"sarcastic\", sarcastic_assistant)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"analyze\")\n",
    "builder.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    route_by_sentiment,\n",
    "    {\n",
    "        \"helpful\": \"helpful\",\n",
    "        \"sarcastic\": \"sarcastic\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"helpful\", END)\n",
    "builder.add_edge(\"sarcastic\", END)\n",
    "\n",
    "# Compile\n",
    "graph = builder.compile()\n",
    "\n",
    "# Visualize\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Test and Experiment (10 min)\n",
    "\n",
    "### Test with Different Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"Could you please help me track my order?\",\n",
    "    \"I need assistance with my account, thank you.\",\n",
    "    \"This is ridiculous! Fix it NOW!\",\n",
    "    \"Your service is terrible! I demand a refund!\",\n",
    "    \"What are your business hours?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ðŸ“ Customer Question: \\\"{question}\\\"\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = graph.invoke({\"question\": question, \"messages\": []})\n",
    "    \n",
    "    print(f\"\\nðŸ’¬ Final Response:\")\n",
    "    print(f\"   {result['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Cases and Refinement\n",
    "\n",
    "Try questions that are borderline or mixed sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_cases = [\n",
    "    \"I'm frustrated but trying to stay calm. Can you help?\",\n",
    "    \"Please help... this is urgent!\",\n",
    "    \"I appreciate your service, but this issue needs immediate attention.\"\n",
    "]\n",
    "\n",
    "for question in edge_cases:\n",
    "    print(f\"\\nðŸ¤” Testing: \\\"{question}\\\"\")\n",
    "    result = graph.invoke({\"question\": question, \"messages\": []})\n",
    "    print(f\"   Routed as: {'polite' if result['is_polite'] else 'rude'}\")\n",
    "    print(f\"   Response preview: {result['answer'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Exercise: Build an Intent-Based Router\n",
    "\n",
    "**Challenge:** Create a customer support router that routes based on INTENT instead of sentiment.\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "1. **Intent Detection Node**: Classify questions into:\n",
    "   - \"technical\" - Technical issues, bugs, errors\n",
    "   - \"billing\" - Payment, refunds, invoices\n",
    "   - \"general\" - General questions, info requests\n",
    "\n",
    "2. **Three Response Nodes**:\n",
    "   - Technical Support (detailed, step-by-step)\n",
    "   - Billing Support (professional, policy-focused)\n",
    "   - General Support (friendly, informative)\n",
    "\n",
    "3. **Conditional Routing**: Route based on detected intent\n",
    "\n",
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE\n",
    "class IntentRouterState(MessagesState):\n",
    "    question: str\n",
    "    intent: NotRequired[str]\n",
    "    answer: NotRequired[str]\n",
    "\n",
    "# INTENT DETECTION NODE\n",
    "class IntentDetector:\n",
    "    def __init__(self, llm):\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Classify the user's question into one of these intents:\n",
    "            - technical: Technical issues, bugs, errors, system problems\n",
    "            - billing: Payments, refunds, invoices, pricing\n",
    "            - general: General questions, information requests, other\n",
    "            \n",
    "            Respond with ONLY the intent label (technical, billing, or general).\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "    \n",
    "    def __call__(self, state: IntentRouterState) -> IntentRouterState:\n",
    "        # TODO: Implement intent detection\n",
    "        return {\"intent\": \"general\"}  # TODO: Replace with actual detection\n",
    "\n",
    "# RESPONSE NODES\n",
    "class TechnicalSupport:\n",
    "    def __init__(self, llm):\n",
    "        # TODO: Create prompt for technical support\n",
    "        self.chain = None  # TODO\n",
    "    \n",
    "    def __call__(self, state: IntentRouterState) -> IntentRouterState:\n",
    "        # TODO: Implement technical response\n",
    "        return {\"answer\": \"Technical support response\"}\n",
    "\n",
    "# TODO: Implement BillingSupport and GeneralSupport classes\n",
    "\n",
    "# ROUTING FUNCTION\n",
    "def route_by_intent(state: IntentRouterState) -> Literal[\"technical\", \"billing\", \"general\"]:\n",
    "    # TODO: Implement routing logic\n",
    "    return \"general\"\n",
    "\n",
    "# BUILD GRAPH\n",
    "# TODO: Create the graph with intent-based routing\n",
    "\n",
    "# Test questions\n",
    "test_questions_intent = [\n",
    "    \"My app keeps crashing when I click the submit button\",\n",
    "    \"I was charged twice for my subscription\",\n",
    "    \"What are your business hours?\",\n",
    "    \"How do I reset my password?\",\n",
    "    \"Can I get a refund for last month?\"\n",
    "]\n",
    "\n",
    "# TODO: Test the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "âœ… **LLM-Based Routing**: Use LLMs to make intelligent, context-aware routing decisions  \n",
    "âœ… **Sentiment/Intent Analysis**: Classify input before processing  \n",
    "âœ… **Multiple Personalities**: Different nodes can have distinct behaviors  \n",
    "âœ… **Prompt Engineering**: The quality of your prompts determines routing accuracy  \n",
    "âœ… **Edge Cases**: Always test borderline cases to refine routing logic  \n",
    "\n",
    "### When to Use LLM-Based Routing\n",
    "\n",
    "**âœ“ Use LLM routing when:**\n",
    "- Decision requires understanding context or nuance\n",
    "- Multiple classification categories exist\n",
    "- Input is unstructured natural language\n",
    "- Rules-based logic becomes too complex\n",
    "\n",
    "**âœ— Avoid LLM routing when:**\n",
    "- Simple boolean logic suffices\n",
    "- Performance/cost is critical\n",
    "- Deterministic behavior required\n",
    "- Input is already structured\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Module 1.8: Build a tool-calling agent with ReAct pattern\n",
    "- Learn how agents can take actions, not just make decisions\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [LangGraph Conditional Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [LangChain Prompts](https://python.langchain.com/docs/concepts/prompts/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}