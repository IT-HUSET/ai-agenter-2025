{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9af2cedbcec94d",
   "metadata": {},
   "source": [
    "# Simple _**agentic**_ RAG demo\n",
    "\n",
    "Or more precisely: **CRAG** (_Corrective RAG_) - a technique for a little bit smarter RAG\n",
    "\n",
    "<a target=\"_blank\" href=\"https://githubtocolab.com/IT-HUSET/ai-agenter-2025/blob/main/exercises/langgraph/extras/simple-rag-agent-demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>\n",
    "\n",
    "![Corrective RAG](https://github.com/IT-HUSET/ai-workshop-250121/blob/main/images/crag-flow.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf534f63f3d850",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f94673c1cabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai~=2.0 httpx~=0.28.1 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 docarray~=0.41.0 pypdf~=6.1 --upgrade --quiet\n",
    "%pip install chromadb~=1.1.1 lark~=1.3 --upgrade --quiet\n",
    "%pip install langchain~=0.3 langchain_openai~=0.3 langchain_community~=0.3.31 langchain-chroma~=0.2.6 --upgrade --quiet\n",
    "%pip install langgraph~=0.6 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d510e51c0909d92",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085d7c17c7156a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IN_COLAB = True\n",
    "    # Get API key from Colab secrets\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ Running in Google Colab - API key loaded from secrets\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    # Load from .env file for local development\n",
    "    try:\n",
    "        from dotenv import load_dotenv, find_dotenv\n",
    "        load_dotenv(find_dotenv())\n",
    "        print(\"‚úÖ Running locally - API key loaded from .env file\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ùå OPENAI_API_KEY not found!\")\n",
    "    if IN_COLAB:\n",
    "        print(\"   ‚Üí Click the key icon (üîë) in the left sidebar\")\n",
    "        print(\"   ‚Üí Add a secret named 'OPENAI_API_KEY'\")\n",
    "        print(\"   ‚Üí Toggle 'Notebook access' to enable it\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Create a .env file with: OPENAI_API_KEY=your-key-here\")\n",
    "else:\n",
    "    print(\"‚úÖ API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5192ae79a33f11",
   "metadata": {},
   "source": [
    "### Setup Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6092d8a5ac3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e945bc7bc2583e",
   "metadata": {},
   "source": [
    "## Setup ingestion / retrieval pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116bb16b204d989b",
   "metadata": {},
   "source": [
    "### Setup vector DB (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d222640ccefee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "persist_directory = './db/simle_rag_agent_demo/'\n",
    "\n",
    "# Optionally remove the directory and all files in it recursively if it exists\n",
    "# import shutil\n",
    "# import os\n",
    "# if os.path.exists(persist_directory):\n",
    "#     shutil.rmtree(persist_directory)\n",
    "\n",
    "vectordb: Chroma = Chroma(\n",
    "    collection_name=\"simle_rag_agent_demo\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory # Optionally persist the database\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2bb279f787706",
   "metadata": {},
   "source": [
    "### Setup a text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bdfcbd420696d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 80\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee64e2b269762e9",
   "metadata": {},
   "source": [
    "### Setup documents to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1319121a51e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents to load (tuple of document_id and document_url)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DocInfo:\n",
    "    id: str\n",
    "    url: str\n",
    "\n",
    "documents_to_load: list[DocInfo] = [\n",
    "    DocInfo(\"1\", \"https://data.riksdagen.se/fil/B9E2F955-31EA-4E9E-91EB-9AE0A3A8FFA7\"), # F√∂rordning om artificiell intelligens, 2020/21:FPM109\n",
    "    DocInfo(\"2\", \"https://data.riksdagen.se/fil/C40BB689-7E23-4593-BDC6-DBEE327C00C6\"), # Risker och m√∂jligheter med artificiell intelligens, 2022/23:374\n",
    "    DocInfo(\"3\", \"https://data.riksdagen.se/fil/4C47740C-D13E-4E22-80CB-43DD1E101080\"), # Direktiv om skadest√•ndsansvar g√§llande artificiell intelligens,\n",
    "    #DocInfo(\"10\", \"https://data.riksdagen.se/fil/BECC9F0F-3DA1-4F44-9417-02DF027DA29C\"), # VITBOK Om artificiell intelligens - en EU-strategi f√∂r spetskompetens och f√∂rtroende# 2022/23:FPM8\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d3524dfc1ec9a",
   "metadata": {},
   "source": [
    "### Ingest - split and add to vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31138108665f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import time\n",
    "\n",
    "def ingest_documents(doc_info: DocInfo):\n",
    "    '''Helper function to ingest a document into the vector database'''\n",
    "\n",
    "    # Check if document already exists\n",
    "    existing = vectordb.get(where={\"doc_id\": doc_info.id})\n",
    "    if existing[\"documents\"]:\n",
    "        print(f\"Document {doc_info.id} already exists in index\")\n",
    "        return\n",
    "\n",
    "    # Load\n",
    "    print(f\"Loading document {doc_info.id} ({doc_info.url})...\")\n",
    "    loader = PyPDFLoader(doc_info.url)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        page.metadata[\"doc_id\"] = doc_info.id\n",
    "\n",
    "    # Split\n",
    "    doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "    # Add to index\n",
    "    print(f\"Adding document {doc_info.id} ({doc_info.url}) to index...\")\n",
    "\n",
    "    # Add in batches, with delay, to avoid rate limiting\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(doc_splits), batch_size):\n",
    "        batch = doc_splits[i:i + batch_size]\n",
    "        vectordb.add_documents(documents=batch)\n",
    "        print(f\"Added splits {i} to {i + batch_size}\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"Added document {doc_info.id} ({doc_info.url}) ({len(pages)} pages) - {len(doc_splits)} splits\")\n",
    "\n",
    "\n",
    "for doc_info in documents_to_load:\n",
    "    ingest_documents(doc_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1dc3a4f6d9fb17",
   "metadata": {},
   "source": [
    "## Setup query graph / pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83c677f583ec19",
   "metadata": {},
   "source": [
    "### Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f33014df9438d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import  List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class GraphState(MessagesState):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    irrelevant_docs: bool\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b3adb4e0bee5",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a970e8996661772",
   "metadata": {},
   "source": [
    "#### Retrieval (Vector Store similarity search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370fb2ff04b4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalNode:\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---RETRIEVE---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        # Retrieval\n",
    "        documents = retriever.invoke(question)\n",
    "\n",
    "        print(f\"---RETRIEVED {len(documents)} DOCS---\")\n",
    "        #print(f\"{documents}\")\n",
    "\n",
    "        return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1443e1fe2ebfb1",
   "metadata": {},
   "source": [
    "#### Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2af74da9518cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class RetrievalGraderNode:\n",
    "    system_template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "    Give a binary score '1' or '0' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "    **Retrieved document:** \\n\\n {document}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # Score each doc\n",
    "        filtered_docs = []\n",
    "        irrelevant_docs = True\n",
    "\n",
    "        for (i, d) in enumerate(documents):\n",
    "            grade = self.chain.invoke(\n",
    "                {\"question\": question, \"document\": d.page_content}\n",
    "            )\n",
    "            if \"1\" in grade:\n",
    "                print(f\"---GRADE: DOCUMENT {i} RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "                irrelevant_docs = False\n",
    "            else:\n",
    "                print(f\"---GRADE: DOCUMENT {i} NOT RELEVANT---\")\n",
    "                continue\n",
    "\n",
    "        return {\"documents\": filtered_docs, \"irrelevant_docs\": irrelevant_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e73d628553e25",
   "metadata": {},
   "source": [
    "#### Web Search (in case irrelevnat relevant docs were found)\n",
    "\n",
    "As a fallback when there are no/few relevant docs, we can use a web search tool to find more information. In this case, we'll use a fake web search node (LLM call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d6bcf9bf8a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeWebSearchNode:\n",
    "    system_template = \"\"\"You are a helpful and cheerful assistant.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chain = self.prompt | llm.bind(temperature=1.0) | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---FAKE WEB SEARCH---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        web_results = self.chain.invoke({\"question\": question})\n",
    "\n",
    "        print(f\"---FAKE WEB SEARCH RESULT: \\n{web_results}\")\n",
    "\n",
    "        web_results = [Document(page_content=web_results)]\n",
    "\n",
    "        return {\"documents\": web_results, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1476bcc382c5f",
   "metadata": {},
   "source": [
    "#### RAG Generation (LLM call with factual/grounded context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33440df4763d1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGNode:\n",
    "    system_template = \"\"\"You are an helpful assistant, expert in answering questions based on provided sources (snippets from documents) and citing the sources used to generate the answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
    "    ALWAYS respond in the SAME language as the original question.\n",
    "\n",
    "    ** Context (snippets from documents): **\n",
    "\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---GENERATE---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # RAG generation - setup context (i.e. relevant documents snippets)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "        # RAG generation - generate answer\n",
    "        answer = self.chain.invoke({\"question\": question, \"context\": context})\n",
    "        #print(f\"---GENERATE - ANSWER: \\n{answer}\")\n",
    "\n",
    "        return {\"documents\": documents, \"answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605d784ca5da455",
   "metadata": {},
   "source": [
    "### Conditional edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a592b41a440c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    irrelevant_docs: bool = state[\"irrelevant_docs\"]\n",
    "\n",
    "    if irrelevant_docs:\n",
    "        print(\n",
    "            \"---DECISION: USE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"fallback\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3947d8187653a",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04bc48f05ca2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Graph ####\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", RetrievalNode())  # retrieve\n",
    "workflow.add_node(\"grade_documents\", RetrievalGraderNode())  # grade documents\n",
    "workflow.add_node(\"web_search\", FakeWebSearchNode())  # failed to find matches\n",
    "workflow.add_node(\"generate\", RAGNode())  # generate\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"fallback\": \"web_search\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e36939b57669f",
   "metadata": {},
   "source": [
    "## Use Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e461d67c05d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"Har det i riksdagen diskuterats n√•got om risker kring anv√§ndningen av artificiell intelligens (AI)?\"\n",
    "    #\"question\": \"Vilka var nobelpristagarna 2023?\" # Should result in web search\n",
    "    #\"question\": \"Vad inneb√§r vitboken om artificiell intelligens?\" # Should NOT result in web search\n",
    "}\n",
    "\n",
    "# Execute graph\n",
    "result = graph.invoke(inputs)\n",
    "\n",
    "print(f\"--- ANSWER: ---\\n{result['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
