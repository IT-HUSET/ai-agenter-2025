{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c18766a23c40cad",
   "metadata": {},
   "source": [
    "# LangGraph - A simple Agent\n",
    "\n",
    "<a target=\"_blank\" href=\"https://githubtocolab.com/IT-HUSET/ai-agenter-2025/blob/main/exercises/langgraph/1.6-langgraph-agent-example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>\n",
    "\n",
    "This notebook demonstrates building simple **Agents** using LangGraph.\n",
    "\n",
    "![Let's build an agent](https://github.com/IT-HUSET/ai-workshop-250121/blob/main/images/llm-apps-2024.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec4d2323fea145",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8fe4232df0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai~=2.0 httpx~=0.28.1 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 docarray~=0.41.0 pypdf~=6.1 --upgrade --quiet\n",
    "%pip install chromadb~=1.1.1 lark~=1.3 --upgrade --quiet\n",
    "%pip install langchain~=0.3 langchain_openai~=0.3 langchain_community~=0.3.31 langchain-chroma~=0.2.6 --upgrade --quiet\n",
    "%pip install langgraph~=0.6 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c393583f81a66",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c06c5d20c9d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IN_COLAB = True\n",
    "    # Get API key from Colab secrets\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ Running in Google Colab - API key loaded from secrets\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    # Load from .env file for local development\n",
    "    try:\n",
    "        from dotenv import load_dotenv, find_dotenv\n",
    "        load_dotenv(find_dotenv())\n",
    "        print(\"‚úÖ Running locally - API key loaded from .env file\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ùå OPENAI_API_KEY not found!\")\n",
    "    if IN_COLAB:\n",
    "        print(\"   ‚Üí Click the key icon (üîë) in the left sidebar\")\n",
    "        print(\"   ‚Üí Add a secret named 'OPENAI_API_KEY'\")\n",
    "        print(\"   ‚Üí Toggle 'Notebook access' to enable it\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Create a .env file with: OPENAI_API_KEY=your-key-here\")\n",
    "else:\n",
    "    print(\"‚úÖ API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78763cd8b82c62",
   "metadata": {},
   "source": [
    "### Setup Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b83462515a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0ad033feccc88",
   "metadata": {},
   "source": [
    "## Let's try some tool calling and build an _actual_ **agent**!\n",
    "\n",
    "![Router](https://github.com/IT-HUSET/ai-workshop-250121/blob/main/images/tool-calling.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a163c72112022",
   "metadata": {},
   "source": [
    "### We begin by defining our \"tools\"\n",
    "Tools can be anything from internal / external APIs, logic within the app, databases lookups, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3d71fb046590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def iceland_vacation_suggestion(topic: Literal['cafes', 'volcanoes', 'activities', 'other']) -> str:\n",
    "    \"\"\"Suggest a vacation spot in Iceland based on the topic. If the user doesn't state a topic, use the topic 'other'.\n",
    "\n",
    "    Args:\n",
    "        topic: The topic of interest. Must be one of 'cafes', 'volcanoes', 'activities', or 'other'.\n",
    "    \"\"\"\n",
    "    print(f\"--- iceland_vacation_suggestion called with {topic} ---\")\n",
    "\n",
    "    if topic == \"cafes\":\n",
    "        return \"Kaffibarinn\"\n",
    "    elif topic == \"volcanoes\":\n",
    "        return \"Fagradalsfjall\"\n",
    "    elif topic == \"activities\":\n",
    "        return \"Inside the Volcano\"\n",
    "    else:\n",
    "        return \"Harpa\"\n",
    "\n",
    "def iceland_vacation_spot_to_avoid(topic: Literal['cafes', 'volcanoes', 'activities', 'other']) -> str:\n",
    "    \"\"\"Suggest a vacation spot to avoid in Iceland, based on the topic. If the user doesn't state a topic, use the topic 'other'. 'other-.\n",
    "\n",
    "    Args:\n",
    "        topic: The topic of interest. Must be one of 'cafes', 'volcanoes', 'activities', or 'other'.\n",
    "    \"\"\"\n",
    "    print(f\"iceland_vacation_spots_to_avoid called with {topic}\")\n",
    "\n",
    "    if topic == \"cafes\":\n",
    "        return \"Cafe Babalu\"\n",
    "    elif topic == \"volcanoes\":\n",
    "        return \"Sundhn√∫kag√≠gar / Grindav√≠k\"\n",
    "    elif topic == \"activities\":\n",
    "        return \"Blue Lagoon\"\n",
    "    else:\n",
    "        return \"Aluminium smelters\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ddf8cbf2563c0",
   "metadata": {},
   "source": [
    "### Next, we need to let the LLM know about our tools\n",
    "\n",
    "Some things to note:\n",
    "1. We bind the tools to the LLM, that is to say, we define the schema our tools and pass it to the LLM so it knows how to call them. The function `bind_tools` is a helper method that turns a list of functions into a **[JSON schema](http://json-schema.org)** that the LLM can understand.\n",
    "2. We set `parallel_tool_calls=False` to ensure that the tools are called sequentially. This is important when the tools have side effects or need to be called in a specific order. And in this case, it make the example a bit clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20059a3caf2538f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [iceland_vacation_suggestion, iceland_vacation_spot_to_avoid]\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1a47bf40bae6b",
   "metadata": {},
   "source": [
    "### Now, we define our \"assistant\" node\n",
    "\n",
    "This time, we'll simply use a simple function to define our node.\n",
    "Note, that this time, we use the predefined **`MessagesState`** instead of defining our own state object. MessageState is a simple state object with a single key, `messages`, which is a list of `AnyMessage` (base class to all message types) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fee2c50dc4af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with tourist information assistance about Iceland.\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2724809c3676f7e",
   "metadata": {},
   "source": [
    "### We can now build our graph\n",
    "\n",
    "Two things to note below:\n",
    "1. We use the predefined **`ToolNode`** for our tool calling node. This takes care of executing the actual tool/function based upon information in the LLM response about a tool call.\n",
    "2. We use the predefined **`tools_condition`** for our conditional edge. This will route the control flow to the tool calling node if the LLM returns information about a tool call in its response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8256723890dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "# NOTE: Here we use the predefined tools_condition for our conditional edge\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08beb2c81cce61",
   "metadata": {},
   "source": [
    "### Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b07318758f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Hi! I'd like do a cool activity in Iceland!\")]\n",
    "#messages = [HumanMessage(content=\"Hi! I'm visiting Iceland next year and would like to do something fun and visit a volcano!\")]\n",
    "#messages = [HumanMessage(content=\"Can you suggest a good caf√© I should go to when I visit Iceland? And is there any place I should avoid?\")]\n",
    "messages = react_graph.invoke({\"messages\": messages})\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
