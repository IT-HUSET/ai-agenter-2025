{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 1.8: Tool Calling Agent with LangGraph\n\n**Duration:** 60 minutes  \n**Objective:** Build a real ReAct agent with tool calling capabilities\n\nIn this exercise, you'll learn:\n- Defining tools using `@tool` decorator\n- Binding tools to LLM with `bind_tools()`\n- Using `ToolNode` for automatic tool execution\n- Implementing `tools_condition` for routing\n- Building a complete ReAct loop\n\n<a target=\"_blank\" href=\"https://githubtocolab.com/IT-HUSET/ai-agents-course-2025/blob/main/exercises/langgraph/1.3-langgraph-tool-calling.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n---\n\n## Setup\n\n### Install dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai~=1.57 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 --upgrade --quiet\n",
    "%pip install langchain~=0.3 langchain_openai~=0.2 --upgrade --quiet\n",
    "%pip install langgraph~=0.2 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Check if running in Google Colab\ntry:\n    from google.colab import userdata\n    IN_COLAB = True\n    # Get API key from Colab secrets\n    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n    print(\"✅ Running in Google Colab - API key loaded from secrets\")\nexcept ImportError:\n    IN_COLAB = False\n    # Load from .env file for local development\n    try:\n        from dotenv import load_dotenv, find_dotenv\n        load_dotenv(find_dotenv())\n        print(\"✅ Running locally - API key loaded from .env file\")\n    except ImportError:\n        print(\"⚠️ python-dotenv not installed. Install with: pip install python-dotenv\")\n\n# Verify API key is set\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    print(\"❌ OPENAI_API_KEY not found!\")\n    if IN_COLAB:\n        print(\"   → Click the key icon (🔑) in the left sidebar\")\n        print(\"   → Add a secret named 'OPENAI_API_KEY'\")\n        print(\"   → Toggle 'Notebook access' to enable it\")\n    else:\n        print(\"   → Create a .env file with: OPENAI_API_KEY=your-key-here\")\nelse:\n    print(\"✅ API key configured!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Define Tools (15 min)\n",
    "\n",
    "### What are Tools?\n",
    "\n",
    "Tools are functions that an agent can call to:\n",
    "- Access external data (APIs, databases)\n",
    "- Perform calculations\n",
    "- Execute actions (send email, create file)\n",
    "- Interact with systems\n",
    "\n",
    "### The `@tool` Decorator\n",
    "\n",
    "LangChain provides a `@tool` decorator that:\n",
    "1. Converts a Python function into a tool\n",
    "2. Generates a JSON schema from docstring and type hints\n",
    "3. Makes the tool compatible with LLM function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import Literal\n",
    "import random\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a city.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to get weather for\n",
    "        \n",
    "    Returns:\n",
    "        A string describing the current weather\n",
    "    \"\"\"\n",
    "    print(f\"🌤️  Calling weather API for: {city}\")\n",
    "    \n",
    "    # Simulated weather data\n",
    "    weather_data = {\n",
    "        \"stockholm\": \"5°C, cloudy with a chance of meatballs\",\n",
    "        \"london\": \"12°C, rainy (as usual)\",\n",
    "        \"tokyo\": \"18°C, clear skies\",\n",
    "        \"new york\": \"8°C, windy\",\n",
    "        \"sydney\": \"25°C, sunny\"\n",
    "    }\n",
    "    \n",
    "    city_lower = city.lower()\n",
    "    if city_lower in weather_data:\n",
    "        return f\"The weather in {city} is: {weather_data[city_lower]}\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {city}. Try Stockholm, London, Tokyo, New York, or Sydney.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression as a string (e.g., \"2 + 2\", \"10 * 5\")\n",
    "        \n",
    "    Returns:\n",
    "        The result of the calculation as a string\n",
    "    \"\"\"\n",
    "    print(f\"🔢 Calculating: {expression}\")\n",
    "    \n",
    "    try:\n",
    "        # Safe evaluation (restricted to basic math)\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating '{expression}': {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_random_fact(topic: Literal[\"space\", \"ocean\", \"animals\", \"history\"]) -> str:\n",
    "    \"\"\"Get a random interesting fact about a topic.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to get a fact about (space, ocean, animals, or history)\n",
    "        \n",
    "    Returns:\n",
    "        An interesting fact as a string\n",
    "    \"\"\"\n",
    "    print(f\"📚 Fetching random fact about: {topic}\")\n",
    "    \n",
    "    facts = {\n",
    "        \"space\": [\n",
    "            \"A day on Venus is longer than a year on Venus.\",\n",
    "            \"Neutron stars are so dense that a teaspoon would weigh billions of tons.\",\n",
    "            \"There are more stars in the universe than grains of sand on all Earth's beaches.\"\n",
    "        ],\n",
    "        \"ocean\": [\n",
    "            \"The ocean produces more than 50% of the world's oxygen.\",\n",
    "            \"The deepest part of the ocean is nearly 7 miles down.\",\n",
    "            \"More people have been to the moon than to the deepest part of the ocean.\"\n",
    "        ],\n",
    "        \"animals\": [\n",
    "            \"Octopuses have three hearts and blue blood.\",\n",
    "            \"A group of flamingos is called a 'flamboyance'.\",\n",
    "            \"Honey never spoils. Archaeologists have found 3000-year-old honey that's still edible.\"\n",
    "        ],\n",
    "        \"history\": [\n",
    "            \"Cleopatra lived closer to the Moon landing than to the building of the pyramids.\",\n",
    "            \"Oxford University is older than the Aztec Empire.\",\n",
    "            \"The first programmer was Ada Lovelace in the 1840s.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return random.choice(facts[topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tools Independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weather tool\n",
    "print(get_weather.invoke(\"Stockholm\"))\n",
    "print()\n",
    "\n",
    "# Test calculator\n",
    "print(calculate.invoke(\"42 * 17\"))\n",
    "print()\n",
    "\n",
    "# Test fact tool\n",
    "print(get_random_fact.invoke(\"space\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Tool Schema\n",
    "\n",
    "See how LangChain converts your function into a tool schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"Weather Tool Schema:\")\n",
    "print(json.dumps(get_weather.args_schema.schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Build Tool-Calling Graph (30 min)\n",
    "\n",
    "### The ReAct Pattern\n",
    "\n",
    "**ReAct** = **Rea**soning + **Act**ing\n",
    "\n",
    "The agent follows this loop:\n",
    "1. **Think**: Decide what to do based on the question\n",
    "2. **Act**: Call a tool if needed\n",
    "3. **Observe**: See the tool's result\n",
    "4. **Repeat** or **Answer**: Continue until done\n",
    "\n",
    "### Bind Tools to LLM\n",
    "\n",
    "First, we \"bind\" our tools to the LLM, which tells it what tools are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tools\n",
    "tools = [get_weather, calculate, get_random_fact]\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)\n",
    "\n",
    "print(f\"✅ Bound {len(tools)} tools to LLM\")\n",
    "print(f\"   Tools: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Assistant Node\n",
    "\n",
    "The assistant node calls the LLM with tools. It uses **MessagesState** which includes conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# System message defines the assistant's behavior\n",
    "system_message = SystemMessage(\n",
    "    content=\"\"\"You are a helpful assistant with access to tools.\n",
    "    \n",
    "    When users ask questions:\n",
    "    1. Think about what information you need\n",
    "    2. Use the appropriate tool to get that information\n",
    "    3. Provide a helpful, friendly answer based on the tool results\n",
    "    \n",
    "    Available tools:\n",
    "    - get_weather: Get current weather for a city\n",
    "    - calculate: Evaluate mathematical expressions\n",
    "    - get_random_fact: Get interesting facts about topics\n",
    "    \n",
    "    Always use tools when you need information. Don't make up data.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def assistant_node(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"The main assistant node that calls LLM with tools.\"\"\"\n",
    "    print(\"\\n🤖 Assistant thinking...\")\n",
    "    \n",
    "    # Call LLM with tools\n",
    "    response = llm_with_tools.invoke([system_message] + state[\"messages\"])\n",
    "    \n",
    "    # Check if tool calls were made\n",
    "    if response.tool_calls:\n",
    "        print(f\"   📞 Calling {len(response.tool_calls)} tool(s)\")\n",
    "        for tc in response.tool_calls:\n",
    "            print(f\"      - {tc['name']}({tc['args']})\")\n",
    "    else:\n",
    "        print(\"   💬 Responding to user (no tools needed)\")\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ToolNode for Automatic Execution\n",
    "\n",
    "`ToolNode` automatically:\n",
    "1. Extracts tool calls from the LLM response\n",
    "2. Executes the appropriate tool functions\n",
    "3. Returns the results as ToolMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Create tool node with our tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "print(\"✅ Created ToolNode for automatic tool execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph with tools_condition\n",
    "\n",
    "`tools_condition` is a built-in conditional function that:\n",
    "- Routes to \"tools\" if the LLM made tool calls\n",
    "- Routes to END if the LLM provided a final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"assistant\", assistant_node)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"assistant\")\n",
    "\n",
    "# Conditional edge: if tool calls exist, go to tools; otherwise END\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,  # Built-in function that checks for tool calls\n",
    ")\n",
    "\n",
    "# After tools execute, loop back to assistant\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# Compile\n",
    "graph = builder.compile()\n",
    "\n",
    "# Visualize - note the xray=True to see internal structure\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Test and Debug (15 min)\n",
    "\n",
    "### Single Tool Call Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Simple question requiring one tool\n",
    "question = \"What's the weather like in Stockholm?\"\n",
    "\n",
    "print(f\"❓ Question: {question}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=question)]\n",
    "})\n",
    "\n",
    "# Print the conversation\n",
    "print(\"\\n💬 Conversation:\")\n",
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Tool Calls Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex question requiring multiple tools\n",
    "question = \"Calculate 123 * 456 and also tell me the weather in Tokyo\"\n",
    "\n",
    "print(f\"❓ Question: {question}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=question)]\n",
    "})\n",
    "\n",
    "print(\"\\n💬 Conversation:\")\n",
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Tool Calls Test\n",
    "\n",
    "Test if the agent can use tool results to make follow-up tool calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question that requires sequential reasoning\n",
    "question = \"\"\"Calculate 15 * 8, then tell me if that number is greater than 100. \n",
    "If it is, give me a space fact. If not, give me an ocean fact.\"\"\"\n",
    "\n",
    "print(f\"❓ Question: {question}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=question)]\n",
    "})\n",
    "\n",
    "print(\"\\n💬 Final Answer:\")\n",
    "result[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream the Execution\n",
    "\n",
    "See the agent's thought process in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What's the weather in London and give me a history fact\"\n",
    "\n",
    "print(f\"❓ Question: {question}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n🔄 Streaming execution:\\n\")\n",
    "\n",
    "for step in graph.stream({\"messages\": [HumanMessage(content=question)]}):\n",
    "    print(f\"Step: {list(step.keys())[0]}\")\n",
    "    print(f\"  Content: {step}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Exercise: Build a Research Assistant Agent\n",
    "\n",
    "**Challenge:** Create a research assistant with more advanced tools.\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "1. **Create these tools**:\n",
    "   - `search_wikipedia(topic: str)` - Simulate Wikipedia search\n",
    "   - `get_current_date()` - Return today's date\n",
    "   - `convert_currency(amount: float, from_currency: str, to_currency: str)` - Currency conversion\n",
    "\n",
    "2. **Build a ReAct agent** that can:\n",
    "   - Answer questions using multiple tools\n",
    "   - Handle tool failures gracefully\n",
    "   - Provide sourced, accurate answers\n",
    "\n",
    "3. **Test with complex queries**:\n",
    "   - \"What year was Python created and how many years ago was that?\"\n",
    "   - \"Convert 100 USD to EUR and then to JPY\"\n",
    "\n",
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# TOOLS\n",
    "@tool\n",
    "def search_wikipedia(topic: str) -> str:\n",
    "    \"\"\"Search Wikipedia for information about a topic.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "    \"\"\"\n",
    "    # TODO: Implement simulated Wikipedia search\n",
    "    # Create a dict with some topics and return relevant info\n",
    "    wiki_data = {\n",
    "        \"python\": \"Python was created by Guido van Rossum and first released in 1991.\",\n",
    "        \"langchain\": \"LangChain is a framework for developing LLM applications, created in 2022.\",\n",
    "        # Add more topics...\n",
    "    }\n",
    "    return wiki_data.get(topic.lower(), f\"No information found for {topic}\")\n",
    "\n",
    "@tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Get the current date.\"\"\"\n",
    "    # TODO: Return current date in readable format\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "@tool\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Convert currency from one type to another.\n",
    "    \n",
    "    Args:\n",
    "        amount: The amount to convert\n",
    "        from_currency: Source currency (USD, EUR, JPY, etc.)\n",
    "        to_currency: Target currency (USD, EUR, JPY, etc.)\n",
    "    \"\"\"\n",
    "    # TODO: Implement simulated currency conversion\n",
    "    # Use approximate exchange rates\n",
    "    rates = {\n",
    "        (\"USD\", \"EUR\"): 0.92,\n",
    "        (\"EUR\", \"USD\"): 1.09,\n",
    "        (\"USD\", \"JPY\"): 149.50,\n",
    "        # Add more rates...\n",
    "    }\n",
    "    \n",
    "    pair = (from_currency.upper(), to_currency.upper())\n",
    "    if pair in rates:\n",
    "        result = amount * rates[pair]\n",
    "        return f\"{amount} {from_currency} = {result:.2f} {to_currency}\"\n",
    "    return f\"Conversion rate not available for {from_currency} to {to_currency}\"\n",
    "\n",
    "# TODO: Build the agent graph with these tools\n",
    "# research_tools = [search_wikipedia, get_current_date, convert_currency]\n",
    "# ...\n",
    "\n",
    "# TODO: Test with complex queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "✅ **Tool Definition**: Use `@tool` decorator for automatic schema generation  \n",
    "✅ **Tool Binding**: `bind_tools()` tells LLM what tools are available  \n",
    "✅ **ToolNode**: Automatically executes tools based on LLM output  \n",
    "✅ **tools_condition**: Built-in routing for tool-calling loops  \n",
    "✅ **ReAct Pattern**: Think → Act → Observe → Repeat  \n",
    "✅ **MessagesState**: Maintains conversation history automatically  \n",
    "\n",
    "### Best Practices for Tool-Calling Agents\n",
    "\n",
    "**Tool Design:**\n",
    "- Clear, descriptive docstrings (LLM reads these!)\n",
    "- Type hints for all parameters\n",
    "- Meaningful parameter names\n",
    "- Handle errors gracefully\n",
    "- Return strings (easiest for LLM to process)\n",
    "\n",
    "**Agent Design:**\n",
    "- Start with system message defining behavior\n",
    "- Use `parallel_tool_calls=False` for debugging\n",
    "- Add logging/printing to understand tool calls\n",
    "- Test tools independently first\n",
    "- Handle tool failures in the system prompt\n",
    "\n",
    "**Debugging:**\n",
    "- Stream execution to see each step\n",
    "- Check tool schemas with `.args_schema.schema()`\n",
    "- Use `xray=True` in graph visualization\n",
    "- Test with increasingly complex queries\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "❌ **Poor docstrings** → LLM doesn't know when to use tool  \n",
    "❌ **Complex return types** → LLM can't interpret results  \n",
    "❌ **No error handling** → Agent gets stuck on failures  \n",
    "❌ **Too many tools** → LLM gets confused about which to use  \n",
    "❌ **Vague tool names** → LLM calls wrong tool  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Day 2: RAG with LangGraph (Module 2.4)\n",
    "- Learn how to combine tools with retrieval\n",
    "- Build production-ready agents\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [LangGraph Tool Calling Guide](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/)\n",
    "- [LangChain Tools Documentation](https://python.langchain.com/docs/concepts/tools/)\n",
    "- [ToolNode API Reference](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode)\n",
    "- [ReAct Paper (2022)](https://arxiv.org/abs/2210.03629)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}